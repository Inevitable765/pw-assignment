{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hBbndE49G-D"
      },
      "outputs": [],
      "source": [
        "1. Write Python scripts for basic file operations and data processing?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def read_file(filename):\n",
        "    with open(filename, 'r') as f:\n",
        "        return f.readlines()\n",
        "\n",
        "def clean_data(lines):\n",
        "    return [line.strip().lower() for line in lines if line.strip()]\n",
        "\n",
        "def write_file(filename, lines):\n",
        "    with open(filename, 'w') as f:\n",
        "        for line in lines:\n",
        "            f.write(line + '\\n')\n",
        "\n",
        "\n",
        "input_file = 'input.txt'\n",
        "output_file = 'output.txt'\n",
        "\n",
        "raw_lines = read_file(input_file)\n",
        "processed_lines = clean_data(raw_lines)\n",
        "write_file(output_file, processed_lines)\n",
        "\n",
        "print(f\"Processed {len(processed_lines)} lines and saved to {output_file}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "gK1xMA429LOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Develop a simple web scraper to extract data from a website?"
      ],
      "metadata": {
        "id": "baMaafrR9V8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "def scrape_titles(url):\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "    titles = []\n",
        "    for heading in soup.find_all('h2'):\n",
        "        titles.append(heading.text.strip())\n",
        "\n",
        "    return titles\n",
        "\n",
        "url = 'https://example-blog.com'\n",
        "titles = scrape_titles(url)\n",
        "\n",
        "for i, title in enumerate(titles, 1):\n",
        "    print(f\"{i}. {title}\")\n"
      ],
      "metadata": {
        "id": "LtXANco79bDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}